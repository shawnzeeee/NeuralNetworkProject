{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project: Machine Learning \n",
    "\n",
    "- <a href='#part1'>Part 1: Deep L-Layer Neural Network for Image Classification</a>\n",
    "\t- You will use pre-built functions to build an L-Layer neural network for an image classification task\n",
    "- <a href='#part2'>Part 2: Full Machine Learning Project</a>\n",
    "\t- You will go through the full \"idea, code, experiment\" cycle to build and improve a model of your choice\n",
    "\n",
    "You may work in groups of 1-3 students for this project.\n",
    "\n",
    "In this project, especially in Part 2, you are expected to show the work you have done in the form of including results for models you have experimented with on the path to the best-performing model. Make sure you include Python and markdown boxes explaining and discussing any decisions you have made and interpretations of the results you have achieved. You can include diagrams, tables, and/or graphs using markdown. **A significant portion of your grade will be based on the progression of your model, not just the final result.**\n",
    "\n",
    "**Note**: All work you submit must be the work of your group. Projects will be checked against each other, and against any work submitted in previous semesters where a similar project was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "## Part 1: Deep L-Layer Neural Network for Image Classification\n",
    "\n",
    "You will use the functions given to you to build a deep L-layer network, and apply it to cat vs non-cat classification. Hopefully, you will see an improvement in accuracy relative to your previous logistic regression implementation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import all the packages that you will need during this assignment. \n",
    "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/index.html) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- nn_functions provides the functions you need to build an L-layer network.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from nn_functions import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Dataset\n",
    "\n",
    "You will use the same \"Cat vs non-Cat\" dataset as in your previous assignment. The model you had built had 70% test accuracy on classifying cats vs non-cats images. Hopefully, your new model will perform a better!\n",
    "\n",
    "**Problem Statement**: You are given a dataset containing:\n",
    "- a training set of m_train images labelled as cat (1) or non-cat (0)\n",
    "- a test set of m_test images labelled as cat and non-cat\n",
    "- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB).\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 50\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.\n",
    "\n",
    "<img src=\"images/imvectorkiank.png\" style=\"width:450px;height:300px;\">\n",
    "\n",
    "<caption><center> <u>Figure 1</u>: Image to vector conversion. <br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12,288$ equals $64 \\times 64 \\times 3$ which is the size of one reshaped image vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Architecture of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with the dataset, it is time to build a deep neural network to distinguish cat images from non-cat images.\n",
    "\n",
    "Here is a simplified network representation for an L-layer neural network:\n",
    "\n",
    "<img src=\"images/LlayerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 2</u>: L-layer neural network.</center></caption> \n",
    "\n",
    "The model can be summarized as: ***[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID***</center></caption>\n",
    "\n",
    "<u>Detailed Architecture of figure 2</u>:\n",
    "- The input is a (64,64,3) image which is flattened to a vector of size (12288,1).\n",
    "- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[1]}$. The result is called the linear unit.\n",
    "- Next, you take the relu of the linear unit. This process could be repeated several times for each $(W^{[l]}, b^{[l]})$ depending on the model architecture.\n",
    "- Finally, you take the sigmoid of the final linear unit. If it is greater than 0.5, you classify it to be a cat.\n",
    "\n",
    "<u>General methodology</u>\n",
    "\n",
    "As usual you will follow the Deep Learning methodology to build the model:\n",
    "1. Initialize parameters / Define hyperparameters\n",
    "2. Loop for num_iterations:\n",
    "    - Forward propagation\n",
    "    - Compute cost function\n",
    "    - Backward propagation\n",
    "    - Update parameters (using parameters, and grads from backprop) \n",
    "4. Use trained parameters to predict labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - L-layer Neural Network\n",
    "\n",
    "**Exercise**: Use the helper functions in the nn_functions file to build an $L$-layer neural network with the following structure: **[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID**. Spend some time looking through the functions and understanding how they can be used to build a deep neural network. The functions you may need and their inputs are:\n",
    "```python\n",
    "def initialize_parameters_deep(layers_dims):\n",
    "    ...\n",
    "    return parameters \n",
    "def L_model_forward(X, parameters):\n",
    "    ...\n",
    "    return AL, caches\n",
    "def compute_cost(AL, Y):\n",
    "    ...\n",
    "    return cost\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    ...\n",
    "    return grads\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now train the model as a 4-layer neural network. \n",
    "\n",
    "Run the cell below to train your model. The cost should decrease on every iteration. It may take a few minutes to run 2500 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! It seems that your 4-layer neural network has better performance than your previous assignment network on the same test set. \n",
    "\n",
    "This is good performance for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.5 - Results Analysis\n",
    "\n",
    "First, let's take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few types of images the model tends to do poorly on include:** \n",
    "- Cat body in an unusual position\n",
    "- Cat appears against a background of a similar color\n",
    "- Unusual cat color and species\n",
    "- Camera Angle\n",
    "- Brightness of the picture\n",
    "- Scale variation (cat is very large or small in image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "You can use your own image and see the output of your model. To do that:\n",
    "1. Add your image to the \"images\" folder\n",
    "2. Change your image's name in the following code\n",
    "3. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## START CODE HERE ##\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "## END CODE HERE ##\n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = Image.open(fname)\n",
    "my_image = np.array(image.resize((num_px, num_px))).reshape((1, num_px*num_px*3)).T\n",
    "my_image = my_image/255.\n",
    "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "## Part 2: Full Machine Learning Project\n",
    "\n",
    "Now you will use all you know about building and training neural networks in an \"idea, code, experiment\" cycle on a data set.\n",
    "\n",
    "### 2.1 - Find a dataset (or datasets)\n",
    "\n",
    "Find an appropriate dataset to work with. Some places to look:\n",
    "- [Kaggle](https://www.kaggle.com/datasets) \n",
    "- [University of California, Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php)\n",
    "\n",
    "You can choose just one, or more than one if you'd like to. Think carefully of the type of task you are trying to accomplish (e.g., classification, regression, etc.). Spend some time analyzing and processing the data. For example, decide how to split the data; should you have separate train, dev, and test sets? Does the data need to be cleaned or adjusted? How should the data be normalized? Any other considerations or adjustments needed for the data?\n",
    "\n",
    "Clearly indicate where you found the dataset(s) you are working with.\n",
    "\n",
    "Show the work you have done analyzing and processing the data in Python boxes in this notebook. There should also be associated markdown boxes discussing what you have observed and what decisions you have made.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work for analyzing and processing the data\n",
    "\n",
    "filename = \"heart.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = np.genfromtxt(filename, delimiter=\",\", dtype=None, encoding=None)\n",
    "\n",
    "# Process dataset\n",
    "attributes = dataset[:1,:].flatten()\n",
    "print(\"There are\", len(attributes), \"attributes:\", attributes)\n",
    "dataset = dataset[1:,:]\n",
    "unique_before_map = []\n",
    "for i in range(0, len(attributes)):\n",
    "    unique_before_map.append(len(np.unique(dataset[:, i])))\n",
    "    print(np.unique(dataset[:, i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MAP CATEGORICAL VALUES #######\n",
    "# Sex\n",
    "dataset[:, 1][dataset[:, 1] == 'M'] = 0\n",
    "dataset[:, 1][dataset[:, 1] == 'F'] = 1\n",
    "# Chest pain type\n",
    "dataset[:, 2][dataset[:, 2] == 'ASY'] = 0\n",
    "dataset[:, 2][dataset[:, 2] == 'ATA'] = 1\n",
    "dataset[:, 2][dataset[:, 2] == 'NAP'] = 2\n",
    "dataset[:, 2][dataset[:, 2] == 'TA'] = 3\n",
    "# Resting ECG\n",
    "dataset[:, 6][dataset[:, 6] == 'LVH'] = 0\n",
    "dataset[:, 6][dataset[:, 6] == 'Normal'] = 1\n",
    "dataset[:, 6][dataset[:, 6] == 'ST'] = 2\n",
    "# Exercise induced Angina\n",
    "dataset[:, 8][dataset[:, 8] == 'N'] = 0\n",
    "dataset[:, 8][dataset[:, 8] == 'Y'] = 1\n",
    "# ST slope\n",
    "dataset[:, 10][dataset[:, 10] == 'Down'] = 0\n",
    "dataset[:, 10][dataset[:, 10] == 'Flat'] = 1\n",
    "dataset[:, 10][dataset[:, 10] == 'Up'] = 2\n",
    "\n",
    "# Sanity check\n",
    "for i in range(0, len(attributes)):\n",
    "    assert(unique_before_map[i] == len(np.unique(dataset[:, i])))\n",
    "print(dataset.shape)\n",
    "\n",
    "# standardize values\n",
    "dataset = np.array(dataset,dtype=float)\n",
    "test = dataset/np.max(dataset, axis=0)\n",
    "for i in range(0, len(attributes)):\n",
    "    print(np.unique(test[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate labels from inputs\n",
    "Y = dataset[:,-1]\n",
    "X = dataset[:,:-1]\n",
    "\n",
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "outcomes = dict(zip(unique, counts))\n",
    "figure(figsize=(6, 6), dpi=80)\n",
    "plt.bar(outcomes.keys(), outcomes.values())\n",
    "plt.show()\n",
    "\n",
    "# Reshape the data to the shape needed for our model\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "train_x = X[:,:800]\n",
    "train_y = Y[:,:800]\n",
    "test_x = X[:,800:]\n",
    "test_y = Y[:,800:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Build your model\n",
    "\n",
    "Start with a basic model, show the results, and then apply whichever improvements you decide to incorporate as per below.\n",
    "\n",
    "You have two options for building your model:\n",
    "- **The difficult option**: Use the provided L-layer network code used above in Part 1 and (later) extend it to incorporate more advanced neural network improvements as given in class\n",
    "- **The easier option**: Use [Keras](https://keras.io) and [TensorFlow](https://www.tensorflow.org) to build a network\n",
    "\t- You may *not* use any framework other than Keras/TensorFlow\n",
    "\n",
    "Ambition will be rewarded! If you choose the easier option, you are expected to incorporate more of the potential improvements given below.\n",
    "\n",
    "Some of the neural network improvements you can consider incorporating for either option (not an exhaustive list):\n",
    "- Weight initialization methods (e.g., zeroes, random, etc.)\n",
    "- Regularization: L2, dropout, etc.\n",
    "- Mini-batch gradient descent\n",
    "- Gradient descent optimization algorithm: momentum, RMSProp, Adam, etc.\n",
    "- Batch normalization\n",
    "\n",
    "Show the results with your model with improvements. Use markdown boxes to discuss the effect of your improvement(s) and change in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work for building a basic model and then applying improvements\n",
    "\n",
    "# layers_dims = [11, 20, 7, 5, 1] #  4-layer model\n",
    "# # L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False)\n",
    "# parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = True)\n",
    "# pred_train = predict(train_x, train_y, parameters)\n",
    "# pred_test = predict(test_x, test_y, parameters)\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(11,), activation='tanh'))\n",
    "model.add(Dense(11, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             metrics='binary_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Idea, Code, Experiment Cycle\n",
    "\n",
    "Now go through a iterative process to improve your model. This will involve things like (not an exhaustive list):\n",
    "- Checking whether you have a bias and/or variance problem. How will you address it? \n",
    "- Hyperparameter tuning: learning rate, # of layers, # of hidden units, activation functions, mini-batch size, etc.\n",
    "- Trying any of the improvements made to the model in part 2.2 to see if it leads to better results\n",
    "\n",
    "We want to see the progression of your model to a final version with the best results you can achieve. You don't have to show results for every single experiment you tried, but a general progression of different models at various stages of development should be included. Feel free to include discussion, diagrams, tables, and/or graphs that may summarize some of your experiments. **If you only show us the final model you've built, your mark will be minimal**.\n",
    "\n",
    "Your discussion should also include details on the methodology you used in your experiments. For example, how did you approach hyperparameter tuning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading**: \n",
    "- Part 1 code for L_layer_model(): **10 marks**\n",
    "- Part 2:\n",
    "  - Dataset choice and analysis: **10 marks**\n",
    "  - Building your model:\n",
    "    - Basic model: **5 marks**\n",
    "    - Implementing improvements to model: **15 marks**\n",
    "  - Idea, Code, Experiment Cycle: **20 marks**\n",
    "\n",
    "**Total** for project: **60 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**: Submit a zip file containing all of the files/folders for your project. Make sure all files are included; do not assume we have certain files already."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
